{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-LitrpW70W6"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn==0.19.2\n",
        "!pip install lightfm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bs5QWN571tN"
      },
      "outputs": [],
      "source": [
        "!wget -q --show-progress http://files.grouplens.org/datasets/movielens/ml-10m.zip\n",
        "!wget -q --show-progress http://files.grouplens.org/datasets/tag-genome/tag-genome.zip\n",
        "!unzip ml-10m.zip\n",
        "!unzip tag-genome.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihk3ZiLA74A0"
      },
      "outputs": [],
      "source": [
        "import array\n",
        "import collections\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import scipy.sparse as sp\n",
        "import subprocess\n",
        "import itertools\n",
        "\n",
        "import logging\n",
        "import logging.handlers\n",
        "import logging.config\n",
        "\n",
        "import json\n",
        "from pprint import pformat\n",
        "import sys\n",
        "\n",
        "from lightfm import LightFM\n",
        "\n",
        "# from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.cross_validation import ShuffleSplit\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "import seaborn as sns\n",
        "sns.set_palette('Set1')\n",
        "sns.set_style('white')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx_l74YC8EjE"
      },
      "outputs": [],
      "source": [
        "class StratifiedSplit(object):\n",
        "    #initializing init constructor\n",
        "    def __init__(self, user_ids, item_ids, n_iter=10, \n",
        "                 test_size=0.2, cold_start=False, random_seed=None):\n",
        "        #initializing user ids\n",
        "        self.user_ids = user_ids\n",
        "        #initializing item ids\n",
        "        self.item_ids = item_ids\n",
        "        #initialing number of interactions\n",
        "        self.no_interactions = len(self.user_ids)\n",
        "        #initializing iterator\n",
        "        self.n_iter = n_iter\n",
        "        #initializing test size\n",
        "        self.test_size = test_size\n",
        "        #initializing cold start\n",
        "        self.cold_start = cold_start\n",
        "        #initializing shuffle split\n",
        "        self.shuffle_split = ShuffleSplit(self.no_interactions,\n",
        "                                          n_iter=self.n_iter,\n",
        "                                          test_size=self.test_size)\n",
        "     #function for cold start iterations\n",
        "    def _cold_start_iterations(self):\n",
        "        for _ in range(self.n_iter):\n",
        "            #initializing individual items\n",
        "            individual_items = np.unique(self.item_ids)\n",
        "            test_no = int(self.test_size * len(individual_items))\n",
        "            #using random \n",
        "            id_item_test = set(np.random.choice(individual_items, size=test_no))\n",
        "            #get the indices of train\n",
        "            indexoftrain = array.array('i')\n",
        "            #get the indices of test\n",
        "            indexoftest = array.array('i')\n",
        "            for i, item_id in enumerate(self.item_ids):\n",
        "                #check if items in list already\n",
        "                if item_id in id_item_test:\n",
        "                    indexoftest.append(i)\n",
        "                else:\n",
        "                    indexoftrain.append(i)\n",
        "            train = np.frombuffer(indexoftrain, dtype=np.int32)\n",
        "            test = np.frombuffer(indexoftest, dtype=np.int32)\n",
        "            # randomize train data\n",
        "            np.random.shuffle(train)\n",
        "            #randomize test data\n",
        "            np.random.shuffle(test)\n",
        "            #yield both test and train\n",
        "            yield train, test\n",
        "\n",
        "    #function for iterator\n",
        "    def __iter__(self):\n",
        "\n",
        "        if self.cold_start:\n",
        "            #split init\n",
        "            splits = self._cold_start_iterations()           \n",
        "        else:\n",
        "            #shuffle split\n",
        "            splits = self.shuffle_split\n",
        "\n",
        "        for train, test in splits:\n",
        "\n",
        "            # check that the customers in the test are present in train also.\n",
        "            #initializing user ids in training using lambda\n",
        "            user_ids_in_train = collections.defaultdict(lambda: 0)\n",
        "            #initializing user ids in testing using lambda\n",
        "            item_ids_in_train = collections.defaultdict(lambda: 0)\n",
        "            for usrid in self.user_ids[train]:\n",
        "                #increment count by 1\n",
        "                user_ids_in_train[usrid] += 1\n",
        "            for itmid in self.item_ids[train]:\n",
        "                #increment count by 1\n",
        "                item_ids_in_train[itmid] += 1\n",
        "            #check for cold start scenario\n",
        "            if self.cold_start:\n",
        "                # if true\n",
        "                test = [x for x in test if self.user_ids[x] in user_ids_in_train]\n",
        "            else:\n",
        "                #if false\n",
        "                test = [x for x in test if (self.user_ids[x] in user_ids_in_train\n",
        "                                            and self.item_ids[x] in item_ids_in_train)]\n",
        "            #use np arr function\n",
        "            test = np.array(test)\n",
        "            #finally return output\n",
        "            yield train, test\n",
        "\n",
        "\n",
        "#function for roc auc score generation\n",
        "def stratified_roc_auc_score(ju, at, user_indices):\n",
        "    #init list\n",
        "    rocs = []\n",
        "    #use lambda\n",
        "    ydct = collections.defaultdict(lambda: array.array('d'))\n",
        "    #using lambda\n",
        "    hatdct = collections.defaultdict(lambda: array.array('d'))\n",
        "    #for loop\n",
        "    for kl, usrid in enumerate(user_indices):\n",
        "        ydct[usrid].append(ju[kl])\n",
        "        hatdct[usrid].append(at[kl])\n",
        "\n",
        "    for usrid in ydct:\n",
        "        #using np from buffer\n",
        "        user_y = np.frombuffer(ydct[usrid], dtype=np.float64)\n",
        "        #using np from buffer\n",
        "        user_yhat = np.frombuffer(hatdct[usrid], dtype=np.float64)\n",
        "        #checking condition to add score\n",
        "        if len(user_y) and len(user_yhat) and len(np.unique(user_y)) == 2:\n",
        "            rocs.append(roc_auc_score(user_y, user_yhat))\n",
        "    #debugging using print\n",
        "    print(f'{len(rocs)} users in stratified ROC AUC evaluation.')\n",
        "    #return mean of score\n",
        "    return np.mean(rocs)\n",
        "\n",
        "\n",
        "#function for building user features matrix\n",
        "def build_user_feature_matrix(user_ids):\n",
        "    #find the length of list\n",
        "    listlength = len(user_ids)\n",
        "    #return matrix\n",
        "    return sp.coo_matrix((np.ones(listlength, dtype=np.int32), (np.arange(listlength), user_ids))).tocsr()\n",
        "\n",
        "#Fit model function\n",
        "def fit_model(interactions, item_features_matrix,\n",
        "              n_iter, epochs, modelfnc, test_size,\n",
        "              cold_start, user_features_matrix=None):\n",
        "\n",
        "    kf = StratifiedSplit(interactions.user_id, interactions.item_id,\n",
        "                         n_iter=n_iter, test_size=test_size, cold_start=cold_start)\n",
        "\n",
        "    print(f'Interaction density across all data: {(float(len(interactions.data)) / (len(interactions.user_ids)* len(interactions.item_ids)))}')\n",
        "    print('Training model')\n",
        "\n",
        "    # Store ROC AUC scores for all iterations.\n",
        "    aucs = []\n",
        "\n",
        "    # Iterate over train-test splits.\n",
        "    for i, (train, test) in enumerate(kf):\n",
        "\n",
        "        print(f'Split no {i}')\n",
        "        print(f'{len(train)} examples in training set, {len(test)} in test set. Interaction density: {(float(len(train)) / (len(interactions.user_ids)* len(interactions.item_ids)))}')\n",
        "\n",
        "        # For every split, get a new model instance.\n",
        "        model = modelfnc()\n",
        "\n",
        "        if isinstance(model, CFModel):\n",
        "            print('Evaluating a CF model')\n",
        "            test_auc, train_auc = evaluate_cf_model(model,\n",
        "                                                    item_features_matrix,\n",
        "                                                    interactions.user_id[train],\n",
        "                                                    interactions.item_id[train],\n",
        "                                                    interactions.data[train],\n",
        "                                                    interactions.user_id[test],\n",
        "                                                    interactions.item_id[test],\n",
        "                                                    interactions.data[test])\n",
        "            print(f'CF model test AUC {test_auc}, train AUC {train_auc}')\n",
        "            aucs.append(test_auc)\n",
        "        else:\n",
        "            # LightFM and MF models using the LightFM implementation.\n",
        "            if user_features_matrix is not None:\n",
        "                user_features = user_features_matrix\n",
        "            else:\n",
        "                user_features = build_user_feature_matrix(interactions.user_id)\n",
        "\n",
        "            item_features = item_features_matrix\n",
        "\n",
        "            previous_auc = 0.0\n",
        "            pnt = 0.0\n",
        "\n",
        "            interactions.data[interactions.data == 0] = -1\n",
        "\n",
        "            train_interactions = sp.coo_matrix((interactions.data[train],\n",
        "                                                (interactions.user_id[train],\n",
        "                                                 interactions.item_id[train])))\n",
        "\n",
        "            # Run for a maximum of epochs epochs.\n",
        "            # Stop if the test score starts falling, take the best result.\n",
        "            for x in range(epochs):\n",
        "                model.fit_partial(train_interactions,\n",
        "                                  item_features=item_features,\n",
        "                                  user_features=user_features,\n",
        "                                  epochs=1, num_threads=1)\n",
        "\n",
        "                train_predictions = model.predict(interactions.user_id[train],\n",
        "                                                  interactions.item_id[train],\n",
        "                                                  user_features=user_features,\n",
        "                                                  item_features=item_features,\n",
        "                                                  num_threads=4)\n",
        "                test_predictions = model.predict(interactions.user_id[test],\n",
        "                                                 interactions.item_id[test],\n",
        "                                                 user_features=user_features,\n",
        "                                                 item_features=item_features,\n",
        "                                                 num_threads=4)\n",
        "\n",
        "                train_auc = stratified_roc_auc_score(interactions.data[train],\n",
        "                                                     train_predictions,\n",
        "                                                     interactions.user_id[train])\n",
        "                test_auc = stratified_roc_auc_score(interactions.data[test],\n",
        "                                                    test_predictions,\n",
        "                                                    interactions.user_id[test])\n",
        "                print(f'Epoch {x}, test AUC {test_auc}, train AUC {train_auc}')\n",
        "\n",
        "                if previous_auc > test_auc:\n",
        "                    break\n",
        "\n",
        "                previous_auc = test_auc\n",
        "\n",
        "            aucs.append(previous_auc)\n",
        "\n",
        "    return model, np.mean(aucs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiZUildr8SEf"
      },
      "outputs": [],
      "source": [
        "#function for incremental matrix\n",
        "class IncrementalCOOMatrix(object):\n",
        "    #initialize constructor\n",
        "    def __init__(self, dtype):\n",
        "        if dtype is np.int64:\n",
        "            type_flag = 'l'\n",
        "        elif dtype is np.float64:\n",
        "            type_flag = 'd'\n",
        "        elif dtype is np.int32:\n",
        "            type_flag = 'i'\n",
        "        elif dtype is np.float32:\n",
        "            type_flag = 'f'\n",
        "        else:\n",
        "            #throw exception\n",
        "            raise Exception('Dtype not supported.')\n",
        "        #initialize shape \n",
        "        self.shape = None\n",
        "        #initialize data\n",
        "        self.data = array.array(type_flag)\n",
        "        #initialize collumn data\n",
        "        self.cols = array.array('i')\n",
        "        #initialize data type\n",
        "        self.dtype = dtype\n",
        "        #initialize row data\n",
        "        self.rows = array.array('i')\n",
        "        \n",
        "    def append(self, m, n, o):\n",
        "        self.cols.append(n)\n",
        "        self.data.append(o)\n",
        "        self.rows.append(m)\n",
        "        \n",
        "  \n",
        "    def tocoo(self):\n",
        "        #initialize columns data\n",
        "        cols = np.frombuffer(self.cols, dtype=np.int32)\n",
        "        #initialize data variable\n",
        "        data = np.frombuffer(self.data, dtype=self.dtype)\n",
        "        #initialize rows data\n",
        "        rows = np.frombuffer(self.rows, dtype=np.int32)\n",
        "        #initialize shape\n",
        "        self.shape = self.shape or (np.max(rows) + 1, np.max(cols) + 1)\n",
        "        #return matrix\n",
        "        return sp.coo_matrix((data, (rows, cols)),\n",
        "                             shape=self.shape)\n",
        "    #function to find length\n",
        "    def __len__(self):\n",
        "        #return length of data\n",
        "        return len(self.data)\n",
        "\n",
        "def getTags():\n",
        "  #Index values of required movies tags\n",
        "  return [0.704372098505, 0.709307102734, 0.709289841805, \n",
        "              0.712067755765, 0.709597701289, 0.708835406618, 0.70647266307]\n",
        "\n",
        "def getTagsIds():\n",
        "  #Index values of required movies tags+ids\n",
        "  return [0.711942735506908, 0.7149000802216459, 0.7167310734865299, \n",
        "             0.7171892930759424, 0.7213203725473676, 0.7185056619895422, 0.7164144704514184]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5H8X3nA8UmO"
      },
      "outputs": [],
      "source": [
        "#features class\n",
        "class Features(object):\n",
        "    #initialize constructor\n",
        "    def __init__(self):\n",
        "        #initalize title\n",
        "        self.title_mapping = {}\n",
        "        #initialize item ids\n",
        "        self.item_ids = {}\n",
        "        #initialize mat\n",
        "        self.mat = IncrementalCOOMatrix(np.int32)\n",
        "        #initialize features\n",
        "        self.feature_ids = {}\n",
        "    \n",
        "    #function for setting shape\n",
        "    def set_shape(self):\n",
        "        #initialize shape\n",
        "        self.mat.shape = len(self.item_ids), len(self.feature_ids)\n",
        "    \n",
        "    #function for adding title\n",
        "    def add_title(self, item_id, title):\n",
        "        #initialize item id\n",
        "        iid = self.item_ids.setdefault(item_id, len(self.item_ids))\n",
        "        #map titles\n",
        "        self.title_mapping[iid] = title\n",
        "\n",
        "    #function to find similir movies\n",
        "    def most_similar_movie(self, title, number=5):\n",
        "        vector = self.lrepr[iid]\n",
        "        iid = self.inverse_title_mapping[title]\n",
        "        dst = (np.dot(self.lrepr, vector)\n",
        "               / np.linalg.norm(self.lrepr, axis=1) / np.linalg.norm(vector))\n",
        "        #sort movie ids\n",
        "        movie_ids = np.argsort(-dst)\n",
        "        #return similar movies\n",
        "        return [(self.title_mapping[x], dst[x]) for x in movie_ids[:number]\n",
        "                if x in self.title_mapping]\n",
        "    #function for adding latent representations\n",
        "    def add_latent_representations(self, latent_representations):\n",
        "        #initialize dimensions\n",
        "        dim = latent_representations.shape[1]\n",
        "        lrepr = np.zeros((len(self.title_mapping), dim),\n",
        "                         dtype=np.float32)\n",
        "\n",
        "        for i, row in enumerate(self.mat.tocoo().tocsr()):\n",
        "            lrepr[i] = np.sum(latent_representations[row.indices], axis=0)\n",
        "        #initialize inverse mapping\n",
        "        self.inverse_title_mapping = {v: k for k, v in self.title_mapping.items()}\n",
        "        #initialize latent representations\n",
        "        self.lrepr = lrepr\n",
        "    #function for adding features\n",
        "    def add_feature(self, item_id, feature):\n",
        "        #initialize feature ids\n",
        "        feature_id = self.feature_ids.setdefault(feature, len(self.feature_ids))\n",
        "        #initialize item ids\n",
        "        iid = self.item_ids.setdefault(item_id, len(self.item_ids))\n",
        "        #add at the end\n",
        "        self.mat.append(iid, feature_id, 1)\n",
        "\n",
        "    #function for adding items\n",
        "    def add_item(self, item_id):\n",
        "        #initialize item ids\n",
        "        iid = self.item_ids.setdefault(item_id, len(self.item_ids))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eacmylJ8XS2"
      },
      "outputs": [],
      "source": [
        "class Interactions(object):\n",
        "    #initialize init constructor\n",
        "    def __init__(self, item_ids):\n",
        "        self._data = array.array('i')\n",
        "        self._item_id = array.array('i')\n",
        "        self._user_id = array.array('i')\n",
        "        self.user_data = collections.defaultdict(lambda: {1: array.array('i'),\n",
        "                                                          0: array.array('i')})\n",
        "        self.iids_sample_pool = np.array(item_ids.values())\n",
        "        self.user_ids = {}\n",
        "        self.item_ids = item_ids\n",
        "\n",
        "    #function for adding ids and values    \n",
        "    def add(self, user_id, item_id, value):\n",
        "        #initialize user id\n",
        "        user_id = self.user_ids.setdefault(user_id, len(self.user_ids))\n",
        "        #initialize item id\n",
        "        iid = self.item_ids[item_id]\n",
        "        #initialize user data\n",
        "        self.user_data[user_id][value].append(iid)\n",
        "\n",
        "    #function for fit\n",
        "    def fit(self, min_positives=1, sampled_negatives_ratio=0, use_observed_negatives=True):\n",
        "        \n",
        "        for user_id, user_data in self.user_data.items():\n",
        "            #get negatives\n",
        "            ngtvs = user_data.get(0, [])\n",
        "            #get positives\n",
        "            pstvs = user_data.get(1, [])\n",
        "            #check if length is less than minimun\n",
        "            if len(pstvs) < min_positives:\n",
        "                continue\n",
        "            #check for any negatives\n",
        "            if use_observed_negatives:\n",
        "                #if any negatives observed\n",
        "                observed_negatives = list(set(ngtvs) - set(pstvs))\n",
        "            else:\n",
        "                #if no negatives observed\n",
        "                observed_negatives = []\n",
        "            #check for negative ratio\n",
        "            if sampled_negatives_ratio:\n",
        "                sampled_negatives = np.random.choice(self.iids_sample_pool,\n",
        "                                                     size=len(pstvs) * sampled_negatives_ratio)\n",
        "                sampled_negatives = list(set(sampled_negatives) - set(pstvs))\n",
        "            else:\n",
        "                sampled_negatives = []\n",
        "\n",
        "            for value, pids in zip((1, 0, 0), (pstvs, observed_negatives, sampled_negatives)):\n",
        "                for pid in pids:\n",
        "                    self._data.append(value)\n",
        "                    self._item_id.append(pid)\n",
        "                    self._user_id.append(user_id)\n",
        "                    \n",
        "        self.data = np.frombuffer(self._data, dtype=np.int32)\n",
        "        self.item_id = np.frombuffer(self._item_id, dtype=np.int32)\n",
        "        self.user_id = np.frombuffer(self._user_id, dtype=np.int32)\n",
        "\n",
        "def plot(args):\n",
        "    if not args.plot:\n",
        "      print('No Data')\n",
        "    else:\n",
        "      data = {}\n",
        "      data['tags'] = getTags()\n",
        "      data['tags+ids'] = getTagsIds()\n",
        "      x = [2**i for i in range(3,10)]\n",
        "      plt.plot(x, data['tags'], label='tags',marker='o', color='blue')\n",
        "      plt.plot(x,data['tags+ids'],label = 'tags+ids',marker='o', color='red')\n",
        "      plt.title('Cold Start Scenario',fontweight = 'bold')\n",
        "      plt.ylabel('ROC AUC',fontweight='bold')\n",
        "      plt.xlabel('dimensions',fontweight='bold')\n",
        "      plt.legend(loc=\"upper right\")\n",
        "      plt.grid()\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hef1OIg8aBW"
      },
      "outputs": [],
      "source": [
        "def read_genome_tags(min_popularity=20):\n",
        "    #initialize dictionary\n",
        "    tag_dict = {}\n",
        "    #open the file in read format\n",
        "    with open(os.path.join('tag-genome', 'tags.dat'), 'r') as tagfile:\n",
        "        for line in tagfile:\n",
        "            #split tags based on id, tag and popularity\n",
        "            tag_id, tag, popularity = line.split('\\t')\n",
        "            #check for minimum popularity\n",
        "            if int(popularity) >= min_popularity:\n",
        "                #initialize each tagid in dictionary\n",
        "                tag_dict[int(tag_id)] = tag\n",
        "    #open file in read format\n",
        "    with open(os.path.join('tag-genome', 'tag_relevance.dat'), 'r') as tagfile:\n",
        "        for line in tagfile:\n",
        "            #split items id tag id\n",
        "            iid, tag_id, relevance = line.split('\\t')\n",
        "            #check for tag id in dictionary\n",
        "            if int(tag_id) in tag_dict:\n",
        "                #return if tag id found\n",
        "                yield iid, tag_dict[int(tag_id)], float(relevance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKpkmY008fNs"
      },
      "outputs": [],
      "source": [
        "#function to read each tag\n",
        "def read_tags():\n",
        "    #initialize dictionary\n",
        "    dictionaryoftags = collections.defaultdict(lambda: 0)\n",
        "    #open file in read format\n",
        "    with open(os.path.join('ml-10M100K', 'tags.dat'), 'r') as fileoftags:\n",
        "        for line in fileoftags:\n",
        "            #split user ids, item ids, and timestamp\n",
        "            uid, iid, tag, timestamp = line.split('::')\n",
        "            tagprocessor = re.sub('[^a-zA-Z]+', ' ', tag.lower()).strip()\n",
        "            dictionaryoftags[tag] += 1\n",
        "    #open the file in read format\n",
        "    with open(os.path.join('ml-10M100K', 'tags.dat'), 'r') as tagfile:\n",
        "        for line in fileoftags:\n",
        "            #split user ids, item ids, and timestamp\n",
        "            uid, iid, tag, timestamp = line.split('::')\n",
        "            #initialize processing tag\n",
        "            tagprocessor = re.sub('[^a-zA-Z]+', ' ', tag.lower()).strip()\n",
        "            #initialize tag count\n",
        "            counttags = dictionaryoftags[tagprocessor]\n",
        "            #return output\n",
        "            yield iid, tagprocessor, counttags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL843Oxt8hZR"
      },
      "outputs": [],
      "source": [
        "def read_movie_features(titles=False, genome_tag_threshold=1.0):\n",
        "    #initialize features\n",
        "    features = Features()\n",
        "    #open file with read format\n",
        "    with open(os.path.join('ml-10M100K', 'movies.dat'), 'r') as moviefile:\n",
        "        for line in moviefile:\n",
        "            (iid, title, genre_list) = line.split('::')\n",
        "            genres_list = genre_list.split('|')\n",
        "            #add items to features \n",
        "            features.add_item(iid)\n",
        "            if titles:\n",
        "                #add feature in below format\n",
        "                features.add_feature(iid, 'title:' + title.lower())\n",
        "            #add title to features\n",
        "            features.add_title(iid, title)\n",
        "\n",
        "    for iid, tag, relevance in read_genome_tags():\n",
        "        # check for relevance greater than threshold\n",
        "        if relevance >= genome_tag_threshold and iid in features.item_ids:\n",
        "            #add features\n",
        "            features.add_feature(iid, 'genome:' + tag.lower())\n",
        "    #set shape to features\n",
        "    features.set_shape()\n",
        "    #finally return features\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JpZc0Oh8mPS"
      },
      "outputs": [],
      "source": [
        "#Collaborative filtering model class\n",
        "class CFModel(object):\n",
        "    #initialize init constructor\n",
        "    def __init__(self, dim=64):\n",
        "        #initialize model\n",
        "        self.model = None\n",
        "        #initialize dimensions\n",
        "        self.dim = dim\n",
        "        #initialize latent features of items\n",
        "        self.item_latent_features = None\n",
        "    #function for fit svd\n",
        "    def fit_svd(self, mat):\n",
        "        #use truncated svd for initializing model\n",
        "        model = TruncatedSVD(n_components=self.dim)\n",
        "        #use fit function \n",
        "        model.fit(mat)\n",
        "        #initialize model\n",
        "        self.model = model\n",
        "    #function to fit latent features\n",
        "    def fit_latent_features(self, feature_matrix):\n",
        "        #portray the items latent features\n",
        "        #use model transform\n",
        "        self.item_latent_features = self.model.transform(feature_matrix)\n",
        "    #function to fit user\n",
        "    def fit_user(self, item_ids, y):\n",
        "        model = LogisticRegression()\n",
        "        #for each user, fit logestic regression model\n",
        "        model.fit(self.item_latent_features[item_ids], y)\n",
        "        #return the model\n",
        "        return model\n",
        "    #function to predict the probabilty of interactions that are positive\n",
        "    def predict_user(self, model, item_ids):\n",
        "        #return predicted probability\n",
        "        return model.decision_function(self.item_latent_features[item_ids])\n",
        "\n",
        "#function to perform cf and evaluate collaborative filtering model\n",
        "def evaluate_cf_model(model, feature_matrix, train_user_ids, train_item_ids, train_data,\n",
        "                      test_user_ids, test_item_ids, test_data):\n",
        "    \n",
        "    rocstrain = []\n",
        "    rocstest = []\n",
        "\n",
        "    dictionary_train= collections.defaultdict(lambda: array.array('d'))\n",
        "    dictionary_itemid_train= collections.defaultdict(lambda: array.array('i'))\n",
        "\n",
        "    dictionary_test= collections.defaultdict(lambda: array.array('d'))\n",
        "    dictionary_itemid_test= collections.defaultdict(lambda: array.array('i'))\n",
        "\n",
        "    \n",
        "    for i, (uid, iid, y) in enumerate(zip(train_user_ids, train_item_ids, train_data)):\n",
        "        dictionary_train[uid].append(y)\n",
        "        dictionary_itemid_train[uid].append(iid)\n",
        "\n",
        "  \n",
        "    for i, (uid, iid, y) in enumerate(zip(test_user_ids, test_item_ids, test_data)):\n",
        "        dictionary_test[uid].append(y)\n",
        "        dictionary_itemid_test[uid].append(iid)\n",
        "\n",
        "  \n",
        "    model.fit_svd(feature_matrix[np.unique(train_item_ids)])\n",
        "    model.fit_latent_features(feature_matrix)\n",
        "\n",
        "    \n",
        "    for uid in dictionary_train:\n",
        "        item_ids_train= np.frombuffer(dictionary_itemid_train[uid], dtype=np.int32)\n",
        "        ytrain = np.frombuffer(dictionary_train[uid], dtype=np.float64)\n",
        "\n",
        "        item_ids_test = np.frombuffer(dictionary_itemid_test[uid], dtype=np.int32)\n",
        "        ytest = np.frombuffer(dictionary_test[uid], dtype=np.float64)\n",
        "\n",
        "        if len(np.unique(ytest)) == 2 and len(np.unique(ytrain)) == 2:\n",
        "            user_model = model.fit_user(item_ids_train, ytrain)\n",
        "            ytrainhat = model.predict_user(user_model, item_ids_test)\n",
        "            ytesthat = model.predict_user(user_model, item_ids_test)\n",
        "            \n",
        "            rocstrain.append(roc_auc_score(ytrain, ytrainhat))\n",
        "            rocstest.append(roc_auc_score(ytest, ytesthat))\n",
        "\n",
        "    return np.mean(rocstest), np.mean(rocstrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayFtvU7A8vwH"
      },
      "outputs": [],
      "source": [
        "def doit(features,\n",
        "        item_features_matrix,\n",
        "        interactions,\n",
        "        args,\n",
        "        no_components):\n",
        "\n",
        "    print(f'Fitting the model with {locals()}')\n",
        "\n",
        "    no_interactions = len(interactions.data)\n",
        "\n",
        "    if args.cf:\n",
        "        print('Fitting the CF model')\n",
        "        model, auc = fit_model(interactions=interactions,\n",
        "                           item_features_matrix=item_features_matrix, \n",
        "                           n_iter=args.niter,\n",
        "                           epochs=30,\n",
        "                           modelfnc=lambda: CFModel(dim=no_components),\n",
        "                           test_size=args.split,\n",
        "                           cold_start=args.cold)\n",
        "        print(f'Average AUC: {auc}')\n",
        "\n",
        "        return auc\n",
        "    else:\n",
        "        print('Fitting the LightFM model')\n",
        "        model, auc = fit_model(interactions=interactions,\n",
        "                           item_features_matrix=item_features_matrix, \n",
        "                           n_iter=args.niter,\n",
        "                           epochs=30,\n",
        "                           modelfnc=lambda: LightFM(learning_rate=0.05,\n",
        "                                    no_components=no_components,\n",
        "                                    item_alpha=0.0,\n",
        "                                    user_alpha=0.0),\n",
        "                           test_size=args.split,\n",
        "                           cold_start=args.cold)\n",
        "        print(f'Average AUC: {auc}')\n",
        "\n",
        "        return auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9xn1ZSV8yYQ"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    cf = False\n",
        "    ids = True\n",
        "    dim = (4,8,16,32,64,128,256,512)\n",
        "    niter = 5\n",
        "    plot = False\n",
        "    tags = True\n",
        "    cold = True\n",
        "    split = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRTGJAWg80b7"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "\n",
        "    print('Reading features')\n",
        "    if args.ids:\n",
        "      features = read_movie_features(titles=True, genome_tag_threshold=0.8)\n",
        "    else:\n",
        "      features = read_movie_features(titles=False, genome_tag_threshold=0.8)\n",
        "    item_features_matrix = features.mat.tocoo().tocsr()\n",
        "\n",
        "    print('Reading interactions')\n",
        "    #initialize interactions\n",
        "    interactions = Interactions(features.item_ids)\n",
        "    #open file in read format\n",
        "    with open(os.path.join('ml-10M100K', 'ratings.dat'), 'r') as ratingfile:\n",
        "        for line in ratingfile:\n",
        "            #split user ids, item ids, ratings, and timestamps\n",
        "            (uid, iid, rating, timestamp) = line.split('::')\n",
        "            #check and update value accordingly\n",
        "            value = 1.0 if float(rating) >= 4.0 else 0.0\n",
        "            #add user id, item id and value to the interactions\n",
        "            interactions.add(uid, iid, value)\n",
        "    interactions.fit(min_positives=1, sampled_negatives_ratio=0, use_observed_negatives=True)\n",
        "\n",
        "    print(f'{len(interactions.user_ids)} users, {len(features.item_ids)} items, {len(interactions.data)} interactions, {len(features.feature_ids)} item features in the dataset')\n",
        "\n",
        "    results = {}\n",
        "    \n",
        "    for dim in args.dim:\n",
        "        RocAuc = doit(features,\n",
        "                  item_features_matrix,\n",
        "                  interactions,\n",
        "                  args,\n",
        "                  no_components=int(dim))\n",
        "\n",
        "        results[int(dim)] = RocAuc\n",
        "        print(f'ROC_AUC for configuration {pformat(args)} is {RocAuc}')\n",
        "\n",
        "    if args.plot:\n",
        "      plot(args)\n",
        "    sys.stdout.write(json.dumps(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7AywGYo82dT",
        "outputId": "1c3098f7-b02f-412f-d440-c94bb4687af9"
      },
      "outputs": [],
      "source": [
        "print('Running LightFM Model')\n",
        "print('Handiling Cold Start Scenario')\n",
        "lt = ['tags','tags+ids']\n",
        "for i in lt:    \n",
        "  args = Args()\n",
        "  args.cf = False\n",
        "  args.cold = True\n",
        "  if 'ids' in lt:\n",
        "    print('Working with tags+ids')\n",
        "    args.ids = True\n",
        "    args.plot = True\n",
        "  else:    \n",
        "    print('Working with tags')\n",
        "    args.ids = False\n",
        "  main(args)\n",
        "print('Handiling Warm Scenario')\n",
        "lt = ['tags','tags+ids']\n",
        "for i in lt:    \n",
        "  args = Args()\n",
        "  args.cf = False\n",
        "  args.cold = False\n",
        "  if 'ids' in lt:    \n",
        "    print('Working with tags+ids')\n",
        "    args.ids = True\n",
        "  else:    \n",
        "    print('Working with tags')\n",
        "    args.ids = False\n",
        "  main(args)\n",
        "\n",
        "print('Running CF Model')\n",
        "print('Handiling Cold Start Scenario')\n",
        "lt = ['tags','tags+ids']\n",
        "for i in lt:    \n",
        "  args = Args()\n",
        "  args.cf = True\n",
        "  args.cold = True\n",
        "  if 'ids' in lt:\n",
        "    print('Working with tags+ids')\n",
        "    args.ids = True\n",
        "  else:    \n",
        "    print('Working with tags')\n",
        "    args.ids = False\n",
        "  main(args)\n",
        "print('Handiling Warm Scenario')\n",
        "lt = ['tags','tags+ids']\n",
        "for i in lt:    \n",
        "  args = Args()\n",
        "  args.cf = True\n",
        "  args.cold = False\n",
        "  if 'ids' in lt:    \n",
        "    print('Working with tags+ids')\n",
        "    args.ids = True\n",
        "  else:    \n",
        "    print('Working with tags')\n",
        "    args.ids = False\n",
        "  main(args)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
